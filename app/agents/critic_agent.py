# app/agents/critic_agent.py
import json
import os
import asyncio
import logging
from typing import Dict, Any

# Corrected imports relative to project root
from app.agents.base_agent import BaseAgent
from app.registry import register, AgentCapability # Assuming registry handles capabilities
from app.utils.status import ResultStatus
from app.utils.justification_logger import log_justification
from app.schemas.agent_input.critic_agent_input import CriticPlanEvaluationInput
from app.schemas.agent_output.critic_agent_output import CriticPlanEvaluationResult

logger = logging.getLogger(__name__)

# Helper to load JSON safely
def load_json_safe(path):
    try:
        if not os.path.exists(path):
            logger.warning(f"File not found: {path}")
            return None
        with open(path, 'r') as f:
            content = f.read()
            if not content:
                logger.warning(f"File is empty: {path}")
                return None
            return json.loads(content)
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {path}: {e}")
        return None
    except Exception as e:
        logger.error(f"Error loading file {path}: {e}")
        return None

@register(
    key="critic",
    name="Critic Agent",
    capabilities=[AgentCapability.VALIDATION, AgentCapability.PLANNING] # Example capabilities
)
class CriticAgent(BaseAgent):
    """Critic agent evaluates plans generated by the Architect agent."""

    async def run(self, payload: CriticPlanEvaluationInput) -> CriticPlanEvaluationResult:
        """
        Evaluates a proposed plan based on intent, beliefs, and creed.
        Logs the evaluation decision and justification.
        """
        logger.info(f"Critic agent received plan evaluation request for loop: {payload.loop_id}")
        
        approved = False # Default to not approved
        justification = "Evaluation failed due to errors."
        confidence_score = 0.5
        status = ResultStatus.ERROR
        error_message = None

        try:
            # 1. Load the proposed plan
            plan_data = load_json_safe(payload.plan_file_path)
            if plan_data is None:
                raise ValueError(f"Failed to load or parse plan file: {payload.plan_file_path}")

            # 2. Load context (beliefs, creed) - Optional
            belief_data = load_json_safe(payload.belief_surface_path) if payload.belief_surface_path else None
            creed_data = load_json_safe(payload.promethios_creed_path) if payload.promethios_creed_path else None

            # 3. Placeholder Evaluation Logic
            # Simple check: Does the plan exist and have steps?
            # In future, compare plan steps against intent, beliefs, creed.
            if isinstance(plan_data, list) and len(plan_data) > 0:
                # Basic check passed
                approved = True
                justification = "Plan structure seems valid (contains steps). Basic approval." 
                confidence_score = 0.75
                status = ResultStatus.SUCCESS
                logger.info(f"Critic: Plan {payload.plan_file_path} passed basic structural check.")
            else:
                approved = False
                justification = "Plan rejected: Invalid structure or empty plan."
                confidence_score = 0.9
                status = ResultStatus.SUCCESS # Evaluation succeeded, but plan rejected
                logger.warning(f"Critic: Plan {payload.plan_file_path} rejected due to invalid structure or being empty.")

            # 4. Log Justification
            action_decision = "Plan Approved" if approved else "Plan Rejected"
            log_justification(
                loop_id=payload.loop_id,
                agent_id="critic",
                action_decision=action_decision,
                justification_text=justification,
                confidence_score=confidence_score
            )

        except ValueError as ve:
            error_message = str(ve)
            logger.error(f"Critic evaluation error for loop {payload.loop_id}: {error_message}")
            justification = f"Evaluation failed: {error_message}"
            confidence_score = 0.2
            status = ResultStatus.ERROR
            # Log failure justification
            log_justification(
                loop_id=payload.loop_id,
                agent_id="critic",
                action_decision="Evaluation Error",
                justification_text=justification,
                confidence_score=confidence_score
            )
        except Exception as e:
            error_message = f"Unexpected error during evaluation: {e}"
            logger.error(f"Critic evaluation unexpected error for loop {payload.loop_id}: {error_message}", exc_info=True)
            justification = f"Evaluation failed unexpectedly: {error_message}"
            confidence_score = 0.1
            status = ResultStatus.ERROR
            # Log failure justification
            log_justification(
                loop_id=payload.loop_id,
                agent_id="critic",
                action_decision="Evaluation Error",
                justification_text=justification,
                confidence_score=confidence_score
            )

        # 5. Return Result
        return CriticPlanEvaluationResult(
            loop_id=payload.loop_id,
            status=status,
            approved=approved,
            justification=justification,
            confidence_score=confidence_score,
            error_message=error_message
        )

