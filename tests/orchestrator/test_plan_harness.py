"""
Plan Test Harness

This module provides a comprehensive test framework to validate loop plans generated by Orchestrator.
It serves as a cognitive unit test system to ensure plans are structurally sound before execution.
"""

import unittest
import json
import os
import sys
from unittest.mock import patch, MagicMock
from datetime import datetime

# Add the parent directory to the path so we can import the modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from orchestrator.modules.plan_from_prompt import plan_from_prompt_driver, generate_plan_from_prompt
from orchestrator.modules.plan_validator import validate_loop_plan, validate_and_log
from orchestrator.modules.tool_predictor import predict_required_tools, check_tool_availability

# Test prompt fixtures
TEST_PROMPTS = [
    # Valid prompts
    "Build a simple reflection app with HAL and CRITIC only.",
    "Create a form submission page with validation.",
    "I want a cosmic poetry writing tool with timeline memory.",
    
    # Incomplete prompts
    "",
    "Build.",
    
    # Ambiguous goals
    "Do the thing with the stuff.",
    "Make it work better.",
    
    # Edge cases
    "Create a CRITIC-only analysis without any code generation.",
    "I need a pure data validation system with no UI components."
]

class TestPlanHarness(unittest.TestCase):
    """Test harness for validating loop plans generated by Orchestrator."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.project_id = "test_project_001"
        self.valid_prompt_indices = [0, 1, 2]  # Indices of valid prompts in TEST_PROMPTS
        self.incomplete_prompt_indices = [3, 4]  # Indices of incomplete prompts
        self.ambiguous_prompt_indices = [5, 6]  # Indices of ambiguous prompts
        self.edge_case_indices = [7, 8]  # Indices of edge case prompts
        
        # Mock memory and chat logging functions
        self.memory_logs = []
        self.chat_logs = []
        self.sandbox_logs = []
    
    @patch('orchestrator.modules.plan_from_prompt.log_to_memory')
    @patch('orchestrator.modules.plan_from_prompt.log_to_chat')
    @patch('orchestrator.modules.plan_from_prompt.log_to_sandbox')
    def test_valid_prompt_generates_valid_plan(self, mock_log_sandbox, mock_log_chat, mock_log_memory):
        """Test that valid prompts generate valid plans."""
        # Set up mocks to capture logs
        mock_log_memory.side_effect = self.mock_log_to_memory
        mock_log_chat.side_effect = self.mock_log_to_chat
        mock_log_sandbox.side_effect = self.mock_log_to_sandbox
        
        # Test each valid prompt
        for idx in self.valid_prompt_indices:
            prompt = TEST_PROMPTS[idx]
            
            # Clear previous logs
            self.memory_logs = []
            self.chat_logs = []
            self.sandbox_logs = []
            
            # Generate plan from prompt
            plan = plan_from_prompt_driver(self.project_id, prompt)
            
            # Assert plan was generated
            self.assertIsNotNone(plan, f"Failed to generate plan for valid prompt: {prompt}")
            
            # Assert plan has required fields
            self.assertIn("loop_id", plan, "Plan missing loop_id field")
            self.assertIn("agents", plan, "Plan missing agents field")
            self.assertIn("goals", plan, "Plan missing goals field")
            self.assertIn("planned_files", plan, "Plan missing planned_files field")
            self.assertIn("confirmed", plan, "Plan missing confirmed field")
            
            # Assert plan passes schema validation
            errors = validate_loop_plan(plan)
            self.assertEqual(len(errors), 0, f"Plan validation failed with errors: {errors}")
            
            # Assert plan was logged to memory
            self.assertTrue(any("loop_plans" in log for log in self.memory_logs), 
                           "Plan was not logged to loop_plans in memory")
            
            # Assert trace was logged to memory
            self.assertTrue(any("orchestrator_traces" in log for log in self.memory_logs), 
                           "Trace was not logged to orchestrator_traces in memory")
            
            # Assert plan was logged to chat
            self.assertTrue(len(self.chat_logs) > 0, "Plan was not logged to chat")
            
            # Assert plan was logged to sandbox
            self.assertTrue(len(self.sandbox_logs) > 0, "Plan was not logged to sandbox")
            
            # Print success message
            print(f"✅ Valid prompt test passed for: {prompt}")
    
    @patch('orchestrator.modules.plan_from_prompt.log_to_memory')
    @patch('orchestrator.modules.plan_from_prompt.log_to_chat')
    def test_incomplete_prompt_fails_gracefully(self, mock_log_chat, mock_log_memory):
        """Test that incomplete prompts fail gracefully."""
        # Set up mocks to capture logs
        mock_log_memory.side_effect = self.mock_log_to_memory
        mock_log_chat.side_effect = self.mock_log_to_chat
        
        # Test each incomplete prompt
        for idx in self.incomplete_prompt_indices:
            prompt = TEST_PROMPTS[idx]
            
            # Clear previous logs
            self.memory_logs = []
            self.chat_logs = []
            
            # Generate plan from prompt
            plan = plan_from_prompt_driver(self.project_id, prompt)
            
            # Assert plan generation failed
            self.assertIsNone(plan, f"Plan was generated for incomplete prompt: {prompt}")
            
            # Assert error was logged to memory
            self.assertTrue(any(
                "orchestrator_warnings" in log or "orchestrator_errors" in log 
                for log in self.memory_logs
            ), "Error was not logged to memory")
            
            # Assert error was logged to chat
            self.assertTrue(len(self.chat_logs) > 0, "Error was not logged to chat")
            
            # Assert error message in chat
            chat_message = self.chat_logs[0].get("message", "")
            self.assertTrue(
                "Failed" in chat_message or "failed" in chat_message or "Error" in chat_message,
                f"Chat message does not indicate failure: {chat_message}"
            )
            
            # Print success message
            print(f"✅ Incomplete prompt test passed for: {prompt}")
    
    @patch('orchestrator.modules.plan_from_prompt.log_to_memory')
    @patch('orchestrator.modules.plan_from_prompt.log_to_chat')
    def test_ambiguous_goal_handling(self, mock_log_chat, mock_log_memory):
        """Test handling of prompts with ambiguous goals."""
        # Set up mocks to capture logs
        mock_log_memory.side_effect = self.mock_log_to_memory
        mock_log_chat.side_effect = self.mock_log_to_chat
        
        # Test each ambiguous prompt
        for idx in self.ambiguous_prompt_indices:
            prompt = TEST_PROMPTS[idx]
            
            # Clear previous logs
            self.memory_logs = []
            self.chat_logs = []
            
            # Generate plan from prompt
            plan = plan_from_prompt_driver(self.project_id, prompt)
            
            # Either the plan should be None (rejected) or it should have warnings
            if plan is None:
                # Assert error was logged to memory
                self.assertTrue(any(
                    "orchestrator_warnings" in log for log in self.memory_logs
                ), "Warning was not logged to memory for ambiguous prompt")
                
                # Assert error was logged to chat
                self.assertTrue(len(self.chat_logs) > 0, "Warning was not logged to chat")
            else:
                # If a plan was generated, ensure it has default goals
                self.assertTrue(len(plan.get("goals", [])) > 0, "Plan has no goals for ambiguous prompt")
                
                # Check if there are warnings in the logs
                has_warnings = any(
                    "orchestrator_warnings" in str(log) for log in self.memory_logs
                )
                
                # Either there should be warnings or the goals should be generic
                generic_goals = ["Process and execute operator request"]
                has_generic_goal = any(goal in plan.get("goals", []) for goal in generic_goals)
                
                self.assertTrue(
                    has_warnings or has_generic_goal,
                    f"No warnings logged and no generic goals for ambiguous prompt. Goals: {plan.get('goals', [])}"
                )
            
            # Print success message
            print(f"✅ Ambiguous prompt test passed for: {prompt}")
    
    @patch('orchestrator.modules.plan_from_prompt.log_to_memory')
    @patch('orchestrator.modules.plan_from_prompt.log_to_chat')
    def test_edge_case_agent_stack(self, mock_log_chat, mock_log_memory):
        """Test handling of prompts with unusual agent stacks."""
        # Set up mocks to capture logs
        mock_log_memory.side_effect = self.mock_log_to_memory
        mock_log_chat.side_effect = self.mock_log_to_chat
        
        # Test each edge case prompt
        for idx in self.edge_case_indices:
            prompt = TEST_PROMPTS[idx]
            
            # Clear previous logs
            self.memory_logs = []
            self.chat_logs = []
            
            # Generate plan from prompt
            plan = plan_from_prompt_driver(self.project_id, prompt)
            
            # Assert plan was generated
            self.assertIsNotNone(plan, f"Failed to generate plan for edge case prompt: {prompt}")
            
            # Assert plan has required fields
            self.assertIn("loop_id", plan, "Plan missing loop_id field")
            self.assertIn("agents", plan, "Plan missing agents field")
            self.assertIn("goals", plan, "Plan missing goals field")
            
            # Check if CRITIC is in the agent stack for the CRITIC-only prompt
            if "CRITIC-only" in prompt or "critic-only" in prompt.lower():
                self.assertIn("critic", plan["agents"], "CRITIC not in agent stack for CRITIC-only prompt")
            
            # Check if HAL is not in the agent stack for the "no code generation" prompt
            if "no code generation" in prompt.lower():
                # Either HAL should not be present, or if present, there should be a warning
                if "hal" in plan["agents"]:
                    self.assertTrue(any(
                        "orchestrator_warnings" in log for log in self.memory_logs
                    ), "No warning logged for HAL in no-code-generation prompt")
            
            # Assert plan passes schema validation
            errors = validate_loop_plan(plan)
            self.assertEqual(len(errors), 0, f"Plan validation failed with errors: {errors}")
            
            # Print success message
            print(f"✅ Edge case agent stack test passed for: {prompt}")
    
    def test_tool_prediction_for_plans(self):
        """Test that tools are correctly predicted for plans."""
        # Generate a plan from a valid prompt
        loop_id = int(datetime.now().timestamp()) % 1000
        plan = generate_plan_from_prompt(TEST_PROMPTS[1], loop_id)  # Form submission prompt
        
        # Predict required tools
        tools = predict_required_tools(plan)
        
        # Assert tools were predicted
        self.assertTrue(len(tools) > 0, "No tools predicted for plan")
        
        # For a form submission page, form_validator should be predicted
        self.assertIn("form_validator", tools, "form_validator not predicted for form submission page")
        
        # Check tool availability
        availability = check_tool_availability(tools)
        
        # Assert availability was checked for all tools
        self.assertEqual(len(availability), len(tools), "Tool availability not checked for all tools")
        
        # Print success message
        print(f"✅ Tool prediction test passed with {len(tools)} tools predicted")
    
    def test_trace_logging_for_plans(self):
        """Test that trace logging works correctly for plans."""
        # Generate a plan from a valid prompt
        loop_id = int(datetime.now().timestamp()) % 1000
        plan = generate_plan_from_prompt(TEST_PROMPTS[0], loop_id)  # Simple reflection app
        
        # Validate and log the plan
        is_valid, errors, trace = validate_and_log(plan)
        
        # Assert plan is valid
        self.assertTrue(is_valid, f"Plan validation failed with errors: {errors}")
        
        # Assert trace was created
        self.assertIsNotNone(trace, "No trace was created")
        self.assertIn("trace_id", trace, "Trace missing trace_id field")
        self.assertIn("action", trace, "Trace missing action field")
        self.assertIn("status", trace, "Trace missing status field")
        self.assertIn("timestamp", trace, "Trace missing timestamp field")
        
        # Assert trace ID format is correct
        expected_trace_id = f"loop_{loop_id}_plan"
        self.assertEqual(trace["trace_id"], expected_trace_id, f"Trace ID {trace['trace_id']} does not match expected {expected_trace_id}")
        
        # Assert trace status is correct
        self.assertEqual(trace["status"], "passed", "Trace status is not 'passed'")
        
        # Print success message
        print(f"✅ Trace logging test passed with trace ID: {trace['trace_id']}")
    
    def mock_log_to_memory(self, project_id, data):
        """Mock function for logging to memory."""
        self.memory_logs.append(data)
    
    def mock_log_to_chat(self, project_id, message):
        """Mock function for logging to chat."""
        self.chat_logs.append(message)
    
    def mock_log_to_sandbox(self, project_id, loop_plan):
        """Mock function for logging to sandbox."""
        self.sandbox_logs.append(loop_plan)

if __name__ == "__main__":
    unittest.main()
